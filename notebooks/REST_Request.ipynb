{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import sys, os\n",
    "\n",
    "# Set the environment variables from shell environment\n",
    "OPENAI_API_KEY = \"Your_Key\"\n",
    "OPENAI_CHAT_MODEL = \"solidrust/Codestral-22B-v0.1-hf-AWQ\"\n",
    "OPENAI_CHAT_API_URL = \"https://isdm-chat.crocc.meso.umontpellier.fr/openai\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=OPENAI_CHAT_MODEL,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=OPENAI_CHAT_API_URL,\n",
    ")\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 1 for Entity extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_agent1():\n",
    "    template = \"\"\"\n",
    "    {context}\n",
    "    input question: {question}\n",
    "\n",
    "    You are an entity extraction expert. Given an input question, you must identify the entities present in the question based on the values I provide. Return only the entities that exist in the input question and their values.\n",
    "\n",
    "############################################################################################################################################################################\n",
    "\n",
    "    The entity current_intention takes only these values: [Agriculture, Forestry, CONVERSATION, Renewable energy power plants, INDUSTRY, LAND_SPECULATION, MINING, OIL_GAS_EXTRACTION, TOURISM]\n",
    "\n",
    "    The entity crops takes only values: [\"Accacia\", \"Alfalfa\", \"Seaweed\", \"Macroalgae\", \"Almond\", \"Aloe Vera\", \"Apple\", \"Aquaculture\", \"Bamboo\", \"Banana\", \"Bean\", \"Bottle Gourd\", \"Barley\", \"Buckwheat\", \"Cacao\", \"Cassava\", \"Cashew\", \"Chat\", \"Cherries\", \"Canola\", \"Coconut\", \"Coffee Plant\", \"Cotton\", \"Cereals\", \"Corn (Maize)\", \"Croton\", \"Castor Oil Plant\", \"Citrus Fruits\", \"Dill\", \"Eucalyptus\", \"Flowers\", \"Fig-Nut\", \"Fodder Plants\", \"Food crops\", \"Fruit\", \"Grapes\", \"Grains\", \"Herbs\", \"Jatropha\", \"Lentils\", \"Mango\", \"Mustard\", \"Oats\", \"Oil Seeds\", \"Oleagionous plant\", \"Olives\", \"Onion\", \"Oil Palm\", \"Other crops\", \"Palms\", \"Papaya\", \"Passion fruit\", \"Peanut (groundnut)\", \"Pepper\", \"Peas\", \"Pine\", \"Pineapple\", \"Pulses\", \"Pomegranate\", \"Pongamia Pinnata\", \"Potatoes\", \"Rapeseed\", \"Rice\", \"Rice\", \"Roses\", \"Rubber tree\", \"Rye\", \"Seeds Production\", \"Sesame\", \"Sorghum\", \"Soya Beans\", \"soy\", \"Spices\", \"Sisal\", \"Sugar beet\", \"Sugar Cane\", \"Sugar\", \"Sun Flower\", \"Sweet Potatoes\", \"Tobacco\", \"Tea\", \"Teff\", \"Teak\", \"Tomatoes\", \"Trees\", \"Vegetables\", \"Vineyard\", \"Wheat\", \"Yam\"]\n",
    "\n",
    "    The entity implementation_status only takes these values: [project not started, startup phase, in operation, project abandoned]\n",
    "\n",
    "    The entity negotiation_status only takes these values: [Expression of interest, Under negotiation, Memorandum of understanding, Oral Agreement, Contract signed, Change of ownership, Negotiations failed, Contract cancelled, Contract expired]\n",
    "\n",
    "    The entity mineral_resources takes only these values: [\"Aluminum\", \"Asphaltite\", \"Anthracite\", \"Barite\", \"Basalt\", \"Bauxite\", \"Bentonite\", \"Building materials\", \"Carbon\", \"Chromite\", \"Clay\", \"Coal\", \"Cobalt\", \"Copper\", \"Diamonds\", \"Emerald\", \"Feldspar\", \"Fluoride\", \"Gas\", \"Gold\", \"Granite\", \"Gravel\", \"Heavy Mineral Sands\", \"Ilmenite\", \"Iron\", \"Jade\", \"Lead\", \"Limestone\", \"Lithium\", \"Magnetite\", \"Molybdenum\", \"Manganese\", \"Marble\", \"Nickel\", \"Petroleum\", \"Phosphorous\", \"Platinum\", \"Hydrocarbons\", \"crude oil\", \"Pyrolisis Plant\", \"Rutile\", \"Sand\", \"Silica\", \"Silver\", \"Salt\", \"Stone\", \"Tin\", \"Titanium\", \"Uranium\", \"Zinc\"]\n",
    "\n",
    "    The entity animals takes only these values: [\"Aquaculture (animals)\", \"Beef Cattle\", \"Cattle\", \"Dairy Cattle\", \"Fish\", \"Goats\", \"Other livestock\", \"Pork\", \"Poultry\", \"Sheep\", \"Shrimp\"]\n",
    "\n",
    "    The entity negative_impacts takes only these values: [Environmental degradation, Socio-economic, Cultural loss, Eviction, Displacement, Violence]\n",
    "\n",
    "    The entity former_land_cover only takes these values: [Cropland, Forest land, Pasture, Shrub land/Grassland (Rangeland), Marginal land, Wetland]\n",
    "\n",
    "    The entity nature_of_deal takes only these values: [Outright purchase, Lease, Concession, Exploitation permit / license / concession (for mineral resources), Pure contract farming]\n",
    "\n",
    "    The entity Region takes only these values: [Africa, Oceania, Northern America, Asia, Eastern Europe, Latin America and the Caribbean]\n",
    "\n",
    "    Entity: Country\n",
    "\n",
    "    Entity: year\n",
    "    \n",
    "    Entity: deal_size can takes values in Ha or hectares, and it's for surface of deals \n",
    "    \n",
    "    ################################################################################################################################\n",
    "\n",
    "    Expected result must be in list format like this: [\"entity1\" : \"value\", \"entity2\", \"value\"]. Return just a list and nothing else.\n",
    "    \n",
    "    For example:\n",
    "        input question: What types of investment in Forestry exist in Morocco in 2010?\n",
    "        expected result: [\"current_intention\": \"Forestry\", \"Country\": \"Morocco\", \"year\": \"2010\"]\n",
    "        \n",
    "    #################################################################################################################################\n",
    "    \n",
    "    if you have more than value for a single entity, expected result must be like what we did with this example: \n",
    "    question: \"What types of investment exist in Africa and Asia?\" \n",
    "    response: [\"Region\": \"Africa\", \"Region\": \"Asia\"]\n",
    "\n",
    "   \n",
    "    IMPORTANT: However, since the question does not specify the type of investment (such as Agriculture, Forestry, MINING, etc.), no \"current_intention\" entity is included in the response.\n",
    "   \n",
    "        \"\"\"\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def clean(result):\n",
    "    # Using a regular expression to extract the list\n",
    "    match = re.sub(r'\\\\', '', result)\n",
    "    match = re.search(r\"\\[.*?\\]\", match)\n",
    "    if match is None:\n",
    "        return None\n",
    "\n",
    "    list_str = match.group()\n",
    "\n",
    "    output_string = list_str.replace(\"[\", \"{\").replace(\"]\", \"}\")\n",
    "    output_string = output_string.replace(\"'\", '\"')\n",
    "    output_string = output_string.replace(\"None\", \"null\")\n",
    "\n",
    "    try:\n",
    "        data_dict = json.loads(output_string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction of countries and regions using Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_Data\n",
    "data = get_Data.context\n",
    "data = json.loads(data)\n",
    "countries = get_Data.countries\n",
    "regions = get_Data.regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "def word_embedding(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids).last_hidden_state\n",
    "    return last_hidden_states.mean(dim=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __ Calculate the word embedding for the country and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_embeddings_country(countries):\n",
    "\n",
    "    # Calculate the word embedding for the country \n",
    "    for country in countries:\n",
    "        countrie = country['country.name'].lower()\n",
    "        country_embedding = word_embedding(countrie)\n",
    "        country['country_embedding'] = country_embedding\n",
    "\n",
    "    return countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_embeddings_regions(regions):\n",
    "\n",
    "    # Calculate the word embedding for the region\n",
    "    for region in regions:\n",
    "        reg = region['region.name'].lower()\n",
    "        region_embedding = word_embedding(reg)\n",
    "        region['region_embedding'] = region_embedding\n",
    "\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = calculate_word_embeddings_country(countries)\n",
    "regions   = calculate_word_embeddings_regions(regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __ Calculate the similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarities_Country(input_word):\n",
    "    # Initialize the maximum similarity and the corresponding word\n",
    "    max_similarity = -1\n",
    "    most_similar_word = None\n",
    "    id = None\n",
    "\n",
    "    # Obtain the embedding of the input word\n",
    "    input_word = input_word.lower()\n",
    "    input_embedding = word_embedding(input_word)\n",
    "\n",
    "    # Iterate over each country in the list\n",
    "    for country in countries:\n",
    "        # Compute the cosine similarity between the input word and the current country\n",
    "        similarity = cosine_similarity([input_embedding], [country['country_embedding']])\n",
    "\n",
    "        # Update the maximum similarity and the corresponding word if the current similarity is greater\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_word = country[\"country.name\"]\n",
    "            id = country[\"id\"]\n",
    "\n",
    "    return most_similar_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarities_region(input_word):\n",
    "    # Initialize the maximum similarity and the corresponding word\n",
    "    max_similarity = -1\n",
    "    most_similar_word = None\n",
    "    id = None\n",
    "\n",
    "    # Obtain the embedding of the input word\n",
    "    input_word = input_word.lower()\n",
    "    input_embedding = word_embedding(input_word)\n",
    "\n",
    "    # Iterate over each region in the list\n",
    "    for region in regions:\n",
    "        # Compute the cosine similarity between the input word and the current region\n",
    "        similarity = cosine_similarity([input_embedding], [region['region_embedding']])\n",
    "\n",
    "        # Update the maximum similarity and the corresponding word if the current similarity is greater\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_word = region[\"region.name\"]\n",
    "            id = region[\"id\"]\n",
    "\n",
    "    return most_similar_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receive the information of entities extracted by the first Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FormFields(entity):\n",
    "    list = []\n",
    "    if entity == \"current_intention\" or entity == \"Agriculture\" or entity == \"Forestry\" or entity == \"MINING\":\n",
    "        entity = \"intention_of_investment\"\n",
    "    if entity == \"intention_of_investment\" :\n",
    "        for choice in data[\"data\"][\"formfields\"][\"deal\"][entity][\"choices\"]:\n",
    "            name  = \"intention_of_investment\"\n",
    "            type  = choice[\"label\"]\n",
    "            value = choice[\"value\"]\n",
    "            list.append(f\"[attribut: {name}, label: {type}, value:{value}]\")\n",
    "\n",
    "    if entity == \"crops\" :\n",
    "        for choice in data[\"data\"][\"formfields\"][\"deal\"][entity][\"choices\"]:\n",
    "            name  = \"crops\"\n",
    "            type  = choice[\"label\"]\n",
    "            value = choice[\"value\"]\n",
    "            list.append(f\"[attribut: {name}, label: {type}, value:{value}]\")\n",
    "            \n",
    "    if entity == \"minerals\":\n",
    "        entity = \"mineral_resources\"\n",
    "        for choice in data[\"data\"][\"formfields\"][\"deal\"][entity][\"choices\"]:\n",
    "            name  = \"minerals\"\n",
    "            type  = choice[\"label\"]\n",
    "            value = choice[\"value\"]\n",
    "            list.append(f\"[attribut: {name}, label: {type}, value:{value}]\")\n",
    "\n",
    "    if entity == \"nature_of_deal\" or entity == \"former_land_cover\" or entity == \"negative_impacts\" or entity == \"animals\" or entity == \"mineral_resources\" or entity == \"implementation_status\":\n",
    "        for choice in data[\"data\"][\"formfields\"][\"deal\"][entity][\"choices\"]:\n",
    "            name  = entity\n",
    "            type  = choice[\"label\"]\n",
    "            value = choice[\"value\"]\n",
    "            list.append(f\"[attribut: {name}, label: {type}, value:{value}]\")\n",
    "            \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Country(country_name):\n",
    "    list = []\n",
    "    for country in countries:\n",
    "        if country['country.name'] == country_name:\n",
    "            country_id = country['id']\n",
    "            list.append(f\"[country.id: {country_id}, country.name:{country_name}]\")\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Region(region_name):\n",
    "    list = []\n",
    "    for region in regions:\n",
    "        if region['region.name'] == region_name:\n",
    "            region_id = region['id']\n",
    "            list.append(f\"[ region.id: { region_id}, region.name:{region_name}]\")\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def extract_countries(text,nlp):\n",
    "    # Traiter le texte avec spacy\n",
    "    doc = nlp(text)\n",
    "    # Extraire les entités nommées de type 'GPE' (Geopolitical Entity)\n",
    "    countries = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "    return countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recevoir_Entities(data_dict, question):\n",
    "    all_values = []\n",
    "    # Check for the existence of nodes and retrieve their properties\n",
    "    for entity, value in data_dict.items():\n",
    "        if value != None :\n",
    "            if entity == 'Country' :\n",
    "                value = value.lower()\n",
    "                Similar_Country = similarities_Country(value)\n",
    "                if Similar_Country != None:\n",
    "                        Similar_Country = Country(Similar_Country)\n",
    "                        if Similar_Country:\n",
    "                            all_values.append(Similar_Country)\n",
    "                if question:\n",
    "                    countries = extract_countries(question,nlp)\n",
    "                    for country in countries:\n",
    "                        country = country.lower()\n",
    "                        Country_name2 = similarities_Country(country)\n",
    "                        if Country_name2 is not None and Country_name2 != Similar_Country:\n",
    "                            result = Country(Country_name2)\n",
    "                            if result:\n",
    "                                all_values.append(result)\n",
    "                        else: continue\n",
    "            \n",
    "            elif entity == 'Region' or entity == 'Regions':\n",
    "                region_name = value.lower()\n",
    "                Similar_Region = similarities_region(region_name)\n",
    "                result = Region(Similar_Region)\n",
    "                if result:\n",
    "                    all_values.append(result)   \n",
    "                    \n",
    "            elif entity == \"current_intention\" or entity == \"implementation_status\" or entity == \"crops\" or entity == \"animals\" or entity == \"mineral_resources\" or entity == \"nature_of_deal\" or entity == \"negative_impacts\" or entity == \"former_land_cover\" :\n",
    "                if value != \"INVESTMENT\":\n",
    "                    FormField = FormFields(entity)\n",
    "                    all_values.append(FormField)\n",
    "            \n",
    "            else : continue\n",
    "        \n",
    "    return all_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG: Retrieve similar questions based on the user's question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = 'RAG_Data.xlsx'\n",
    "df = pd.read_excel(file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the transformer model\n",
    "model_mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# Convert descriptions into vectors\n",
    "df['vector'] = df['question'].apply(lambda description: model_mpnet.encode(description).astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FAISS index\n",
    "index = faiss.IndexFlatL2(len(df['vector'][0]))\n",
    "vectors = np.array(df['vector'].tolist())\n",
    "index.add(vectors)\n",
    "\n",
    "# Transform the user input into a vector\n",
    "def transform_user_input(user_input):\n",
    "    return model_mpnet.encode(user_input).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechercher des similarités dans FAISS\n",
    "def search_similarities(vector, top_k=1):\n",
    "    distances, indices = index.search(np.array([vector]), top_k)\n",
    "    matches = [{'metadata': {'question': df['question'][i], 'query': df['REST'][i]}} for i in indices[0]]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 2 for in-context learning to generate the REST request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template_Agent2(top_k_matches, all_values, data_dict):\n",
    "    template = \"\"\" ou are a API REST expert, given the following USER_QUESTION and CONTEXT, Please construct a correct REQUEST !\n",
    "    \n",
    "    USER_QUESTION:\n",
    "    --\n",
    "    {user_input}\n",
    "    --\n",
    "    CONTEXT: \n",
    "    \n",
    "    --\n",
    "    {context} \n",
    "    \n",
    "    #######################################################################################################\n",
    "    This list is so so important, it contain the values of the attributes that you will use in the URL\n",
    "    \n",
    "    \"\"\"\n",
    "    data_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    for value in all_values:\n",
    "        template += f\"\"\"\n",
    "        {value}\n",
    "    \"\"\"\n",
    "    template += \"\"\"\n",
    "   ##############################################################################################################################################################################\n",
    "\n",
    "    some examples of question and their corresponding request\n",
    "  \n",
    "    \"\"\"\n",
    "    # Itérez sur les objets dans top_k_matches\n",
    "    for match in top_k_matches:\n",
    "        query = match['metadata']['query'].replace('{', '{{').replace('}', '}}')\n",
    "        question = match['metadata']['question']\n",
    "        template += f\"\"\"\n",
    "        example question: {question}\n",
    "        response: {query}\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "    template += \"\"\"\n",
    "\n",
    "    Attention: do not do the different values of an attribute like that: 'https://landmatrix.org/api/deals/?country_id=288,710,686' you should do like that:  request: 'https://landmatrix.org/api/deals/?country_id=288,country_id=710,country_id=686'\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def model_extraction(question, llm):\n",
    "    parser = StrOutputParser()\n",
    "    prompt = template_agent1()\n",
    "    Agent = prompt | llm | parser \n",
    "    result = Agent.invoke({\"context\": \"\", \"question\": question})\n",
    "    return result\n",
    "\n",
    "def modele_llm(question, llm, parser, retries=3, delay=5):\n",
    "    data_dict = model_extraction(question, llm)\n",
    "    \n",
    "    # Retry if data_dict is empty or None\n",
    "    if not data_dict or data_dict == [] or data_dict == \"\": # while\n",
    "        data_dict = model_extraction(question, llm)\n",
    "        # time.sleep(delay)  # Introduce a delay between retries\n",
    "    \n",
    "    data_dict = clean(data_dict)\n",
    "    \n",
    "    if data_dict:\n",
    "        all_values = Recevoir_Entities(data_dict, question)\n",
    "    else:\n",
    "        all_values = \"___\"  # Placeholder for no data scenario\n",
    "    \n",
    "    user_vector = transform_user_input(question)\n",
    "    top_k_matches = search_similarities(user_vector, top_k=3)\n",
    "    template = create_template_Agent2(top_k_matches, all_values, data_dict)\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            chain = prompt | llm | parser \n",
    "            result = chain.invoke({\"context\": \"\", \"user_input\": question})\n",
    "            return result, data_dict, all_values\n",
    "        except ValueError as e:\n",
    "            if '504 Gateway Time-out' in str(e):\n",
    "                print(f\"Attempt {attempt + 1} failed with timeout error. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            time.sleep(delay)  # Introduce a delay before retrying\n",
    "    \n",
    "    raise ValueError(\"Max retries exceeded with timeout errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_rest_request(text):\n",
    "    # Pattern pour capturer les URLs avec http ou https entourées par des guillemets simples\n",
    "    url_pattern = r\"'(https?://[^\\s']+)'\"\n",
    "    \n",
    "    # Recherche des URLs dans le texte\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    \n",
    "    # Retourner la première URL trouvée ou None si aucune URL n'est trouvée\n",
    "    return urls[0] if urls else None\n",
    "\n",
    "\n",
    "def REST(URL):\n",
    "    url = URL\n",
    "    headers = {\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Vérifier si la requête a réussi (code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Récupérer les données JSON\n",
    "            data = response.json()\n",
    "\n",
    "            # Faire quelque chose avec les données\n",
    "            return data  # Par exemple, afficher les données\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur de requête: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur de connexion: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction of the existence of more than one country in a REST request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "def split_country_ids(url):\n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    \n",
    "    # Extract the query parameters\n",
    "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Get the country_ids from the query parameters\n",
    "    country_ids = query_params.get('country_id', [])\n",
    "    \n",
    "    # If no country_id, return the original URL\n",
    "    if not country_ids:\n",
    "        return [url]\n",
    "    \n",
    "    # Create a base URL without the query parameters\n",
    "    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\"\n",
    "    \n",
    "    # Generate separate URLs for each country_id\n",
    "    separated_urls = []\n",
    "    for country_id in country_ids:\n",
    "        new_query_params = query_params.copy()\n",
    "        new_query_params['country_id'] = [country_id]  # Update with a single country_id\n",
    "        new_query_string = urllib.parse.urlencode(new_query_params, doseq=True)\n",
    "        new_url = f\"{base_url}?{new_query_string}\"\n",
    "        separated_urls.append(new_url)\n",
    "    \n",
    "    return separated_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_data_from_urls(urls, headers):\n",
    "    combined_data = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            combined_data.extend(data)  # Assuming the JSON response is a list of deals\n",
    "        else:\n",
    "            print(f\"Failed to fetch data from {url}\")\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chat_response( llm, parser,question):\n",
    "    \n",
    "    response, entities, all_values = modele_llm(question, llm, parser, retries=3, delay=5)\n",
    "    if response is None:\n",
    "        response, entities, all_values = modele_llm(question, llm, parser, retries=3, delay=5)        \n",
    "    response = re.sub(r'\\\\', '', response)\n",
    "    urls = extract_first_rest_request(response)\n",
    "    combined_data = REST(urls)\n",
    "        \n",
    "    return urls, combined_data, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, response2, entities = Chat_response(llm, parser, \"question\")\n",
    "separated_urls = split_country_ids(response)\n",
    "headers = {'accept': 'application/json'}\n",
    "combined_data = fetch_data_from_urls(separated_urls, headers)          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
